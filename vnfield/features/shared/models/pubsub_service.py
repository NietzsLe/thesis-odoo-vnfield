# -*- coding: utf-8 -*-

# ===========================================
# =           üì° PUBSUB SERVICE              =
# ===========================================

"""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    üß∞ CH·ª®C NƒÇNG: KAFKA PUBSUB SERVICE       ‚îÇ
‚îÇ                                            ‚îÇ
‚îÇ - Qu·∫£n l√Ω producer v√† consumer Kafka       ‚îÇ
‚îÇ - S·ª≠ d·ª•ng system parameter cho c·∫•u h√¨nh    ‚îÇ
‚îÇ - D·ª±a tr√™n TransientModel cho t√≠nh t·∫°m th·ªùi‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""

import logging
import json
from odoo import models, fields, api, _
from odoo.exceptions import UserError

# üí° NOTE(assistant): Import confluent_kafka ƒë·ªÉ x·ª≠ l√Ω Kafka
try:
    from confluent_kafka import Producer, Consumer, KafkaException, KafkaError
except ImportError:
    Producer = Consumer = KafkaException = KafkaError = None

_logger = logging.getLogger(__name__)


class PubSubService(models.TransientModel):
    """
    üîÑ PubSub Service cho Kafka
    
    Service n√†y cung c·∫•p kh·∫£ nƒÉng:
    - Produce messages ƒë·∫øn Kafka topics
    - Consume messages t·ª´ Kafka topics  
    - Qu·∫£n l√Ω c·∫•u h√¨nh th√¥ng qua system parameters
    """
    
    _name = 'vnfield.pubsub.service'
    _description = 'Kafka PubSub Service'

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚ñ∂ C·∫•u h√¨nh v√† kh·ªüi t·∫°o
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    def _get_kafka_config(self):
        """
        üîß L·∫•y c·∫•u h√¨nh Kafka t·ª´ system parameters
        
        Returns:
            dict: Dictionary ch·ª©a c·∫•u h√¨nh Kafka
        """
        # üí° NOTE(assistant): L·∫•y c√°c system parameter cho c·∫•u h√¨nh Kafka
        config = {}
        
        # Bootstrap servers - required
        bootstrap_servers = self.env['ir.config_parameter'].sudo().get_param(
            'kafka.bootstrap_servers', 'localhost:9092'
        )
        config['bootstrap.servers'] = bootstrap_servers
        
        # Security protocol
        security_protocol = self.env['ir.config_parameter'].sudo().get_param(
            'kafka.security_protocol', 'PLAINTEXT'
        )
        config['security.protocol'] = security_protocol
        
        # SASL mechanism (if using SASL)
        if security_protocol in ['SASL_PLAINTEXT', 'SASL_SSL']:
            sasl_mechanism = self.env['ir.config_parameter'].sudo().get_param(
                'kafka.sasl_mechanism', 'PLAIN'
            )
            config['sasl.mechanism'] = sasl_mechanism
            
            sasl_username = self.env['ir.config_parameter'].sudo().get_param(
                'kafka.sasl_username', ''
            )
            if sasl_username:
                config['sasl.username'] = sasl_username
                
            sasl_password = self.env['ir.config_parameter'].sudo().get_param(
                'kafka.sasl_password', ''
            )
            if sasl_password:
                config['sasl.password'] = sasl_password
        
        # SSL configuration (if using SSL)
        if security_protocol in ['SSL', 'SASL_SSL']:
            ssl_ca_location = self.env['ir.config_parameter'].sudo().get_param(
                'kafka.ssl_ca_location', ''
            )
            if ssl_ca_location:
                config['ssl.ca.location'] = ssl_ca_location
                
            ssl_certificate_location = self.env['ir.config_parameter'].sudo().get_param(
                'kafka.ssl_certificate_location', ''
            )
            if ssl_certificate_location:
                config['ssl.certificate.location'] = ssl_certificate_location
                
            ssl_key_location = self.env['ir.config_parameter'].sudo().get_param(
                'kafka.ssl_key_location', ''
            )
            if ssl_key_location:
                config['ssl.key.location'] = ssl_key_location
        
        # üß™ V√≠ d·ª• c·∫•u h√¨nh:
        # config = {
        #     'bootstrap.servers': 'localhost:9092',
        #     'security.protocol': 'PLAINTEXT'
        # }
        
        return config

    def _check_kafka_availability(self):
        """
        ‚úÖ Ki·ªÉm tra t√≠nh kh·∫£ d·ª•ng c·ªßa Kafka
        
        Raises:
            UserError: N·∫øu confluent_kafka kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t
        """
        if not Producer or not Consumer:
            raise UserError(_(
                "confluent_kafka library is not installed. "
                "Please install it using: pip install confluent-kafka"
            ))

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚ñ∂ Producer Methods  
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def produce_message(self, topic, message, key=None, headers=None):
        """
        üì§ G·ª≠i message ƒë·∫øn Kafka topic
        
        Args:
            topic (str): T√™n topic ƒë·ªÉ g·ª≠i message
            message (str|dict): N·ªôi dung message (s·∫Ω ƒë∆∞·ª£c serialize)
            key (str, optional): Key cho message
            headers (dict, optional): Headers cho message
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        # üîç REVIEW(user): Ki·ªÉm tra t√≠nh kh·∫£ d·ª•ng c·ªßa Kafka
        self._check_kafka_availability()
        
        try:
            # L·∫•y c·∫•u h√¨nh Kafka
            config = self._get_kafka_config()
            
            # üí° NOTE(assistant): Th√™m c·∫•u h√¨nh producer specific
            producer_config = config.copy()
            producer_config.update({
                'acks': self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.producer_acks', 'all'
                ),
                'retries': int(self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.producer_retries', '3'
                )),
                'batch.size': int(self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.producer_batch_size', '16384'
                )),
                'linger.ms': int(self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.producer_linger_ms', '5'
                )),
            })
            
            # T·∫°o Producer instance
            producer = Producer(producer_config)
            
            # üîÅ Serialize message n·∫øu l√† dict
            if isinstance(message, dict):
                message = json.dumps(message, ensure_ascii=False)
            
            # Encode message th√†nh bytes
            if isinstance(message, str):
                message = message.encode('utf-8')
                
            # Encode key n·∫øu c√≥
            if key and isinstance(key, str):
                key = key.encode('utf-8')
            
            # Delivery report callback
            def delivery_report(err, msg):
                """
                üìä Callback ƒë∆∞·ª£c g·ªçi khi message ƒë∆∞·ª£c deliver ho·∫∑c fail
                """
                if err is not None:
                    _logger.error(f'Message delivery failed: {err}')
                else:
                    _logger.info(f'Message delivered to {msg.topic()} [{msg.partition()}]')
            
            # üöÄ Produce message
            producer.produce(
                topic=topic,
                value=message,
                key=key,
                headers=headers,
                callback=delivery_report
            )
            
            # üìù TODO(user): C√≥ th·ªÉ th√™m timeout configuration
            producer.flush(timeout=10)  # Wait t·ªëi ƒëa 10 gi√¢y
            
            _logger.info(f'Successfully produced message to topic: {topic}')
            return True
            
        except KafkaException as e:
            _logger.error(f'Kafka error when producing message: {e}')
            raise UserError(_('Kafka error: %s') % str(e))
        except Exception as e:
            _logger.error(f'Error producing message: {e}')
            raise UserError(_('Error producing message: %s') % str(e))
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚ñ∂ Consumer Methods
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def consume_messages(self, topics, group_id=None, timeout=1.0, max_messages=10, message_handler=None):
        """
        üì• Consume messages t·ª´ Kafka topics
        
        Args:
            topics (list): Danh s√°ch topics ƒë·ªÉ subscribe
            group_id (str, optional): Consumer group ID
            timeout (float): Timeout cho m·ªói poll (seconds)
            max_messages (int): S·ªë l∆∞·ª£ng message t·ªëi ƒëa ƒë·ªÉ consume
            message_handler (callable, optional): Function ƒë·ªÉ x·ª≠ l√Ω t·ª´ng message
                Signature: handler(headers, value, message_info) -> processed_value
                - headers (dict): Message headers
                - value (any): Message value (ƒë√£ decode v√† parse JSON n·∫øu c√≥ th·ªÉ)
                - message_info (dict): Th√¥ng tin metadata c·ªßa message
                - Returns: Gi√° tr·ªã ƒë√£ x·ª≠ l√Ω ho·∫∑c None ƒë·ªÉ b·ªè qua message
            
        Returns:
            list: Danh s√°ch messages ƒë√£ consume (v√† ƒë√£ x·ª≠ l√Ω n·∫øu c√≥ handler)
        """
        # üîç REVIEW(user): Ki·ªÉm tra t√≠nh kh·∫£ d·ª•ng c·ªßa Kafka
        self._check_kafka_availability()
        
        if not topics:
            raise UserError(_('Topics list cannot be empty'))
            
        if not isinstance(topics, list):
            topics = [topics]
            
        try:
            # L·∫•y c·∫•u h√¨nh Kafka
            config = self._get_kafka_config()
            
            # üí° NOTE(assistant): Th√™m c·∫•u h√¨nh consumer specific
            consumer_config = config.copy()
            
            # Group ID - m·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng database name + timestamp
            if not group_id:
                group_id = f"odoo_{self.env.cr.dbname}_{self.env.context.get('uid', 'system')}"
            
            consumer_config.update({
                'group.id': group_id,
                'auto.offset.reset': self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.consumer_auto_offset_reset', 'earliest'
                ),
                'enable.auto.commit': self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.consumer_auto_commit', 'true'
                ).lower() == 'true',
                'session.timeout.ms': int(self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.consumer_session_timeout', '30000'
                )),
                'heartbeat.interval.ms': int(self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.consumer_heartbeat_interval', '10000'
                )),
            })
            
            # T·∫°o Consumer instance
            consumer = Consumer(consumer_config)
            
            # Subscribe to topics
            consumer.subscribe(topics)
            _logger.info(f'Consumer subscribed to topics: {topics} with group: {group_id}')
            _logger.info(f'Consumer config - offset reset: {consumer_config.get("auto.offset.reset", "unknown")}')
            
            messages = []
            consumed_count = 0
            
            _logger.info(f'Starting to consume from topics: {topics}')
            
            # üîÅ Loop ƒë·ªÉ consume messages v·ªõi retry logic
            try:
                import time
                start_time = time.time()
                
                # üìù Load timeout configs from system parameters
                max_total_time_multiplier = int(self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.consumer_max_total_time_multiplier', '10'
                ))
                max_total_time = timeout * max_total_time_multiplier  # Maximum total time to spend consuming
                
                no_message_count = 0
                max_no_message_retries = int(self.env['ir.config_parameter'].sudo().get_param(
                    'kafka.consumer_max_no_message_retries', '3'
                ))  # Retry X times when no message
                
                while consumed_count < max_messages and (time.time() - start_time) < max_total_time:
                    msg = consumer.poll(timeout=timeout)
                    
                    if msg is None:
                        # No message received - retry a few times before giving up
                        no_message_count += 1
                        _logger.debug(f'No message received (attempt {no_message_count}/{max_no_message_retries})')
                        
                        if no_message_count >= max_no_message_retries:
                            _logger.debug('Max retries reached, stopping consume...')
                            break
                        continue  # Try again
                        
                    if msg.error():
                        if msg.error().code() == KafkaError._PARTITION_EOF:
                            # End of partition - kh√¥ng ph·∫£i l·ªói th·∫≠t
                            _logger.debug(f'End of partition reached: {msg.topic()}[{msg.partition()}]')
                            continue
                        else:
                            # L·ªói th·ª±c s·ª±
                            _logger.error(f'Consumer error: {msg.error()}')
                            raise KafkaException(msg.error())
                    
                    # üì® Process message th√†nh c√¥ng
                    try:
                        # Reset no message counter khi c√≥ message
                        no_message_count = 0
                        
                        # Decode message
                        value = msg.value().decode('utf-8') if msg.value() else None
                        key = msg.key().decode('utf-8') if msg.key() else None
                        
                        # Th·ª≠ parse JSON n·∫øu c√≥ th·ªÉ
                        try:
                            if value:
                                value = json.loads(value)
                        except (json.JSONDecodeError, TypeError):
                            # Gi·ªØ nguy√™n string n·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON
                            pass
                        
                        # üèóÔ∏è Prepare headers dictionary
                        headers = dict(msg.headers()) if msg.headers() else {}
                        
                        # üìä Prepare message metadata
                        message_info = {
                            'topic': msg.topic(),
                            'partition': msg.partition(),
                            'offset': msg.offset(),
                            'key': key,
                            'timestamp': msg.timestamp()
                        }
                        
                        # üîß Call message handler if provided
                        processed_value = value  # Default: keep original value
                        handler_success = True
                        
                        if message_handler and callable(message_handler):
                            try:
                                # üéØ Call user-provided handler function
                                _logger.debug(f'Calling message handler for message from {msg.topic()}[{msg.partition()}] offset {msg.offset()}')
                                
                                # Handler signature: handler(headers, value, message_info) -> processed_value
                                processed_result = message_handler(headers, value)
                                
                                # üí° NOTE(assistant): Handler c√≥ th·ªÉ return None ƒë·ªÉ b·ªè qua message
                                if processed_result is None:
                                    _logger.debug(f'Message handler returned None, skipping message from {msg.topic()}[{msg.partition()}] offset {msg.offset()}')
                                    continue  # Skip this message
                                
                                processed_value = processed_result
                                _logger.debug(f'Message handler processed successfully for {msg.topic()}[{msg.partition()}] offset {msg.offset()}')
                                
                            except Exception as handler_error:
                                # üêû FIXME(assistant): Handler error - c√≥ th·ªÉ ch·ªçn skip ho·∫∑c keep original
                                _logger.error(f'Message handler error for {msg.topic()}[{msg.partition()}] offset {msg.offset()}: {handler_error}')
                                
                                # üìù TODO(user): C√≥ th·ªÉ th√™m option ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ skip message khi handler l·ªói
                                # M·∫∑c ƒë·ªãnh: gi·ªØ nguy√™n gi√° tr·ªã g·ªëc v√† ti·∫øp t·ª•c
                                processed_value = value
                                handler_success = False
                        
                        # üì¶ Build final message data
                        message_data = {
                            'topic': msg.topic(),
                            'partition': msg.partition(),
                            'offset': msg.offset(),
                            'key': key,
                            'value': processed_value,  # Use processed value instead of original
                            'original_value': value,   # Keep original value for reference
                            'timestamp': msg.timestamp(),
                            'headers': headers,
                            'handler_applied': message_handler is not None,
                            'handler_success': handler_success
                        }
                        
                        messages.append(message_data)
                        consumed_count += 1
                        
                        # üìù Log message v·ªõi n·ªôi dung trong 1 d√≤ng
                        content_preview = str(processed_value)[:100] + "..." if len(str(processed_value)) > 100 else str(processed_value)
                        handler_status = " [HANDLER_APPLIED]" if message_handler else ""
                        handler_status += " [HANDLER_ERROR]" if message_handler and not handler_success else ""
                        _logger.info(f'Consumed message from {msg.topic()}[{msg.partition()}] offset {msg.offset()}{handler_status} | Content: {content_preview}')
                        
                    except Exception as decode_error:
                        _logger.error(f'Error decoding message: {decode_error}')
                        # Continue v·ªõi message ti·∫øp theo
                        continue
                        
            finally:
                # üßπ Cleanup: Close consumer
                consumer.close()
                
            _logger.info(f'Successfully consumed {len(messages)} messages')
            return messages
            
        except KafkaException as e:
            _logger.error(f'Kafka error when consuming messages: {e}')
            raise UserError(_('Kafka error: %s') % str(e))
        except Exception as e:
            _logger.error(f'Error consuming messages: {e}')
            raise UserError(_('Error consuming messages: %s') % str(e))

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚ñ∂ Utility Methods
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    @api.model
    def test_kafka_connection(self):
        """
        üîå Test k·∫øt n·ªëi ƒë·∫øn Kafka cluster
        
        Returns:
            dict: K·∫øt qu·∫£ test connection
        """
        try:
            self._check_kafka_availability()
            config = self._get_kafka_config()
            
            # T·∫°o AdminClient ƒë·ªÉ test connection
            from confluent_kafka.admin import AdminClient
            
            admin_client = AdminClient(config)
            
            # üìù TODO(user): Th√™m timeout configuration cho metadata
            metadata = admin_client.list_topics(timeout=10)
            
            return {
                'success': True,
                'message': _('Successfully connected to Kafka'),
                'broker_count': len(metadata.brokers),
                'topic_count': len(metadata.topics),
                'topics': list(metadata.topics.keys())
            }
            
        except Exception as e:
            _logger.error(f'Kafka connection test failed: {e}')
            return {
                'success': False,
                'message': _('Failed to connect to Kafka: %s') % str(e),
                'error': str(e)
            }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‚ñ∂ Message Handler Utilities
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    @api.model
    def create_simple_handler(self, processing_func=None, filter_func=None):
        """
        üõ†Ô∏è T·∫°o message handler ƒë∆°n gi·∫£n v·ªõi processing v√† filtering
        
        Args:
            processing_func (callable, optional): Function ƒë·ªÉ x·ª≠ l√Ω value
                Signature: func(value) -> processed_value
            filter_func (callable, optional): Function ƒë·ªÉ filter message
                Signature: func(headers, value, message_info) -> bool
                Return True ƒë·ªÉ keep message, False ƒë·ªÉ skip
                
        Returns:
            callable: Message handler function
        """
        def handler(headers, value, message_info):
            """
            üîß Generated message handler
            """
            try:
                # üîç Apply filter if provided
                if filter_func and callable(filter_func):
                    if not filter_func(headers, value, message_info):
                        # Filter says to skip this message
                        return None
                
                # üîÑ Apply processing if provided
                if processing_func and callable(processing_func):
                    return processing_func(value)
                
                # üìù No processing, return original value
                return value
                
            except Exception as e:
                _logger.error(f'Error in simple handler: {e}')
                # Return original value on error
                return value
        
        return handler

    @api.model
    def create_json_validator_handler(self, required_fields=None, schema_validator=None):
        """
        ‚úÖ T·∫°o message handler ƒë·ªÉ validate JSON schema
        
        Args:
            required_fields (list, optional): Danh s√°ch fields b·∫Øt bu·ªôc
            schema_validator (callable, optional): Function ƒë·ªÉ validate schema
                Signature: func(json_data) -> bool
                
        Returns:
            callable: Message handler function
        """
        def handler(headers, value, message_info):
            """
            ‚úÖ JSON validation message handler
            """
            try:
                # üîç Check if value is dict (JSON parsed)
                if not isinstance(value, dict):
                    _logger.warning(f'Message value is not JSON dict, skipping validation: {type(value)}')
                    return value
                
                # ‚úÖ Check required fields
                if required_fields:
                    missing_fields = [field for field in required_fields if field not in value]
                    if missing_fields:
                        _logger.warning(f'Message missing required fields {missing_fields}, skipping')
                        return None  # Skip message
                
                # üîß Apply custom schema validator
                if schema_validator and callable(schema_validator):
                    if not schema_validator(value):
                        _logger.warning(f'Message failed schema validation, skipping')
                        return None  # Skip message
                
                # ‚úÖ Validation passed
                return value
                
            except Exception as e:
                _logger.error(f'Error in JSON validator handler: {e}')
                return None  # Skip on error
        
        return handler

    @api.model
    def create_transform_handler(self, field_mapping=None, add_metadata=False):
        """
        üîÑ T·∫°o message handler ƒë·ªÉ transform data structure
        
        Args:
            field_mapping (dict, optional): Mapping old_field -> new_field
            add_metadata (bool): C√≥ th√™m metadata v√†o message kh√¥ng
                
        Returns:
            callable: Message handler function
        """
        def handler(headers, value, message_info):
            """
            üîÑ Transform message handler
            """
            try:
                result = value
                
                # üó∫Ô∏è Apply field mapping if provided
                if field_mapping and isinstance(value, dict):
                    result = {}
                    for old_field, new_field in field_mapping.items():
                        if old_field in value:
                            result[new_field] = value[old_field]
                    
                    # üìù Keep unmapped fields
                    for field, val in value.items():
                        if field not in field_mapping and field not in result:
                            result[field] = val
                
                # üìä Add metadata if requested
                if add_metadata:
                    if isinstance(result, dict):
                        result['_kafka_metadata'] = {
                            'topic': message_info['topic'],
                            'partition': message_info['partition'],
                            'offset': message_info['offset'],
                            'timestamp': message_info['timestamp'],
                            'headers': headers
                        }
                    else:
                        # üì¶ Wrap non-dict values
                        result = {
                            'data': result,
                            '_kafka_metadata': {
                                'topic': message_info['topic'],
                                'partition': message_info['partition'],
                                'offset': message_info['offset'],
                                'timestamp': message_info['timestamp'],
                                'headers': headers
                            }
                        }
                
                return result
                
            except Exception as e:
                _logger.error(f'Error in transform handler: {e}')
                return value  # Return original on error
        
        return handler

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚ñ∂ Dependencies v√† Symbol Relationships
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

"""
üîó Ph·ª• thu·ªôc c·ªßa c√°c symbols trong file n√†y:

1. **PubSubService class**:
   - K·∫ø th·ª´a t·ª´: models.TransientModel (Odoo core)
   - Ph·ª• thu·ªôc: confluent_kafka (Producer, Consumer, KafkaException)
   - S·ª≠ d·ª•ng: ir.config_parameter model ƒë·ªÉ l·∫•y system parameters
   - Logger: _logger ƒë·ªÉ ghi log

2. **_get_kafka_config method**:
   - Ph·ª• thu·ªôc: self.env['ir.config_parameter'] 
   - Tr·∫£ v·ªÅ: dict configuration cho Kafka client

3. **produce_message method**:
   - Ph·ª• thu·ªôc: _get_kafka_config(), _check_kafka_availability()
   - S·ª≠ d·ª•ng: confluent_kafka.Producer
   - Callback: delivery_report function

4. **consume_messages method**:
   - Ph·ª• thu·ªôc: _get_kafka_config(), _check_kafka_availability()  
   - S·ª≠ d·ª•ng: confluent_kafka.Consumer
   - X·ª≠ l√Ω: JSON serialization/deserialization
   - ENHANCED: H·ªó tr·ª£ message_handler callback v·ªõi signature:
     handler(headers, value, message_info) -> processed_value
   - Handler c√≥ th·ªÉ return None ƒë·ªÉ skip message
   - Tracking: handler_applied v√† handler_success trong message data

5. **test_kafka_connection method**:
   - Ph·ª• thu·ªôc: _get_kafka_config(), _check_kafka_availability()
   - S·ª≠ d·ª•ng: confluent_kafka.admin.AdminClient
   - M·ª•c ƒë√≠ch: Health check cho Kafka cluster

6. **Message Handler Utilities**:
   - create_simple_handler(): T·∫°o handler v·ªõi processing v√† filtering
   - create_json_validator_handler(): Validate JSON schema v√† required fields
   - create_transform_handler(): Transform data structure v√† add metadata
   - T·∫•t c·∫£ handlers follow signature: handler(headers, value, message_info) -> processed_value

C√°c system parameters ƒë∆∞·ª£c s·ª≠ d·ª•ng:
- kafka.bootstrap_servers: ƒê·ªãa ch·ªâ Kafka brokers
- kafka.security_protocol: Giao th·ª©c b·∫£o m·∫≠t  
- kafka.sasl_*: C·∫•u h√¨nh SASL authentication
- kafka.ssl_*: C·∫•u h√¨nh SSL/TLS
- kafka.producer_*: C·∫•u h√¨nh producer
- kafka.consumer_*: C·∫•u h√¨nh consumer

Message Handler Pattern:
- Input: (headers: dict, value: any, message_info: dict)
- Output: processed_value (any type) ho·∫∑c None ƒë·ªÉ skip
- Error handling: Return original value or None t√πy handler logic
- Metadata: topic, partition, offset, timestamp, key
"""
